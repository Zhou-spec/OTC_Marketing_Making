{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define a model with 3 inputs, with 2 hidden layers, and 1 output layer\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # define the layers\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        # add 5 more layers\n",
    "        self.fc2 = nn.Linear(100, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 128)\n",
    "        self.fc6 = nn.Linear(128, 64)\n",
    "        self.fc7 = nn.Linear(64, 32)\n",
    "        self.fc8 = nn.Linear(32, 16)\n",
    "        self.fc9 = nn.Linear(16, 1)\n",
    "        \n",
    "    \n",
    "\n",
    "        # define the activation function\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "\n",
    "    def forward(self, t, S, q):\n",
    "        # define the forward pass\n",
    "        x = torch.tensor([t, S, q], dtype=torch.float32)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        # add 3 more layers\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc9(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "def h(q, Q):\n",
    "    if q < Q and q > - 1 * Q:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def Gaussian_Policy(t, S, q, net, A, B, Q, z, delta, gamma):\n",
    "    N = len(A)\n",
    "    bid_vector = torch.zeros(N)\n",
    "    ask_vector = torch.zeros(N)\n",
    "    x = torch.tensor([t, S, q], dtype=torch.float32)\n",
    "    for i in range(N):\n",
    "        bid_mean = (A[i] / (2 * B[i])) - (net.forward(t, S, q + z[i]) - net.forward(t, S, q) + z[i] * (S + delta * h(q, Q))) / (2 * z[i])\n",
    "        ask_mean = (A[i] / (2 * B[i])) - (net.forward(t, S, q - z[i]) - net.forward(t, S, q) - z[i] * (S - delta * h(q, Q))) / (2 * z[i]) \n",
    "        variance = gamma / (2 * z[i] * B[i])\n",
    "        std = torch.sqrt(variance)\n",
    "        bid_vector[i] = torch.normal(bid_mean, std)\n",
    "        ask_vector[i] = torch.normal(ask_mean, std)\n",
    "\n",
    "    return bid_vector, ask_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def Stock_Prices_Simulation(T, dt, sigma, S0):\n",
    "    N = int(T / dt)\n",
    "    S = torch.zeros(N)\n",
    "    S[0] = S0\n",
    "    for i in range(1, N):\n",
    "        S[i] = S[i - 1] + sigma * math.sqrt(dt) * torch.randn(1)\n",
    "    return S\n",
    "\n",
    "def Market_Order_Simulation(dt, A, B, Q, z, delta, gamma, net, S, q, t):\n",
    "    N = len(A)\n",
    "    bid_vector, ask_vector = Gaussian_Policy(t, S, q, net, A, B, Q, z, delta, gamma)\n",
    "    buy_orders = torch.zeros(N)\n",
    "    sell_orders = torch.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        bid_intensity = A[i] - B[i] * bid_vector[i]\n",
    "        ask_intensity = A[i] - B[i] * ask_vector[i]\n",
    "        buy_orders[i] = torch.distributions.poisson.Poisson(torch.max(bid_intensity * dt, torch.tensor([0.01]))).sample()\n",
    "        sell_orders[i] = torch.distributions.poisson.Poisson(torch.max(ask_intensity * dt, torch.tensor([0.01]))).sample()\n",
    "\n",
    "    return buy_orders, sell_orders\n",
    "\n",
    "def Train_Data_Simulation(T, dt, sigma, S0, A, B, Q, z, delta, gamma, net):\n",
    "    N = int(T / dt)\n",
    "    S = Stock_Prices_Simulation(T, dt, sigma, S0)\n",
    "    buy_orders = torch.zeros(N, len(A))\n",
    "    sell_orders = torch.zeros(N, len(A))\n",
    "    q = torch.zeros(N)\n",
    "    t = torch.zeros(N)\n",
    "    for i in range(N - 1):\n",
    "        buy_orders[i], sell_orders[i] = Market_Order_Simulation(dt, A, B, Q, z, delta, gamma, net, S[i], q[i], t[i])\n",
    "        for j in range(len(A)):\n",
    "            q[i + 1] += (buy_orders[i][j] - sell_orders[i][j]) * z[j]\n",
    "        q[i + 1] += q[i]\n",
    "        t[i + 1] = t[i] + dt\n",
    "        \n",
    "    return S, buy_orders, sell_orders, q, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function_loss(net, S, q, t, dt):\n",
    "    N = len(q)\n",
    "    loss = torch.zeros(N)\n",
    "    for i in range(N - 1):\n",
    "        loss[i] = (net.forward(t[i + 1], S[i + 1], q[i + 1]) - net.forward(t[i], S[i], q[i])) / dt\n",
    "    return loss\n",
    "\n",
    "def inventory_loss(net, S, q, t, dt, buy_orders, sell_orders, z, delta, Q, A, B):\n",
    "    N = len(q)\n",
    "    loss = torch.zeros(N)\n",
    "    for i in range(N):\n",
    "        for k in range(len(A)):\n",
    "            loss[i] = loss[i] + (buy_orders[i][k] - sell_orders[i][k]) * (z[k] * S[i] - delta * h(q[i], Q))\n",
    "            loss[i] = loss[i] +  (A[k] / (2 * B[k])- (net.forward(t[i], S[i], q[i] + z[k]) - net.forward(t[i], S[i], q[i]) + z[k] * (S[i] + delta * h(q[i], Q))) / (2 * z[k])) * buy_orders[i][k]\n",
    "            loss[i] = loss[i] + (A[k] / (2 * B[k]) - (net.forward(t[i], S[i], q[i] - z[k]) - net.forward(t[i], S[i], q[i]) - z[k] * (S[i] - delta * h(q[i], Q))) / (2 * z[k])) * sell_orders[i][k]\n",
    "    return loss\n",
    "\n",
    "def total_loss(net, S, q, t, dt, buy_orders, sell_orders, z, delta, Q, A, B, gamma):\n",
    "    N = len(S)\n",
    "    K = len(A)\n",
    "    loss = torch.zeros(N)\n",
    "    loss1 = value_function_loss(net, S, q, t, dt)\n",
    "    loss2 = inventory_loss(net, S, q, t, dt, buy_orders, sell_orders, z, delta, Q, A, B)\n",
    "    loss = loss1 + loss2 - gamma * ((K * 1.7981798683) + torch.sum(gamma / (2 * z * B)))\n",
    "    \n",
    "    scalar_loss = 0.5 * torch.sum(loss[:-1] ** 2) * dt\n",
    "    return scalar_loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(3)\n",
    "T = 1\n",
    "dt = 0.01\n",
    "S0 = 100\n",
    "q = 0\n",
    "A = torch.tensor([1, 1, 1])\n",
    "B = torch.tensor([0.01, 0.01, 0.01])\n",
    "Q = 100\n",
    "z = torch.tensor([1, 2, 3])\n",
    "delta = 0.01\n",
    "gamma = 0.02\n",
    "sigma = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.14830017089844\n",
      "90.71587371826172\n",
      "82.81932067871094\n",
      "221.34629821777344\n",
      "83.93938446044922\n",
      "200.03526306152344\n",
      "115.32286834716797\n",
      "89.4981689453125\n",
      "197.84326171875\n",
      "72.1910629272461\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m loss \u001b[39m=\u001b[39m total_loss(net, S, q, t, \u001b[39m0.001\u001b[39m, buy_orders, sell_orders, z, delta, Q, A, B, gamma)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m----> 6\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m      7\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m net\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[1;32mc:\\Users\\specf\\anaconda3\\envs\\qids-2023-comp\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\specf\\anaconda3\\envs\\qids-2023-comp\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "for epoch in range(200):\n",
    "    S, buy_orders, sell_orders, q, t = Train_Data_Simulation(1, 0.01, 0.1, S0, A, B, Q, z, delta, gamma, net)\n",
    "    loss = total_loss(net, S, q, t, 0.001, buy_orders, sell_orders, z, delta, Q, A, B, gamma)\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for param in net.parameters():\n",
    "            param -= 0.001 * param.grad\n",
    "    net.zero_grad()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

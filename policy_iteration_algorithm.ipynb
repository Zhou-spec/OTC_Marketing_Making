{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOw+NKoUZE2QSknnJQc54tj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhou-spec/OTC_Marketing_Making/blob/main/policy_iteration_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV3R6S6xHkqs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class ResidualBlock_Conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock_Conv, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet_Conv(nn.Module):\n",
        "    def __init__(self, intput_size, output_size, input_channels, output_channels, num_blocks, final_act):\n",
        "        super(ResNet_Conv, self).__init__()\n",
        "\n",
        "        self.fc = nn.Linear(intput_size, 2096)\n",
        "        self.conv1 = nn.Conv1d(input_channels, output_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.final_act = final_act\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for _ in range(num_blocks):\n",
        "            self.blocks.append(ResidualBlock_Conv(output_channels, output_channels))\n",
        "\n",
        "        self.conv2 = nn.Conv1d(output_channels, input_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc2 = nn.Linear(2096, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 1024)\n",
        "        self.fc4 = nn.Linear(1024, 512)\n",
        "        self.fc5 = nn.Linear(512, output_size)\n",
        "\n",
        "    def forward(self, t, S, q):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        x = torch.tensor([t, S, q], device = device)\n",
        "        out = self.fc(x)\n",
        "        out = out.unsqueeze(0)\n",
        "        out = out.unsqueeze(0)\n",
        "        out = self.conv1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = out.squeeze()\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc5(out)\n",
        "        out = self.final_act(out)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Stock_Prices_Simulation(T, dt, sigma, S0):\n",
        "    # T: the total time needed\n",
        "    # dt: the time interval\n",
        "    # sigma: the volatility\n",
        "    # S0: the initial stock price\n",
        "    # the output is the simulated stock prices in torch tensor\n",
        "\n",
        "    # let S be on the same device as S0\n",
        "    N = int(T / dt)\n",
        "    device = S0.device\n",
        "    S = torch.zeros(N, device = device)\n",
        "    S[0] = S0\n",
        "    for i in range(1, N):\n",
        "        S[i] = S[i - 1] + sigma * (torch.sqrt(torch.tensor([dt])) * torch.randn(1)).to(device)\n",
        "\n",
        "    return S\n",
        "\n",
        "\n",
        "def Market_Order_Generator(bid_vector, ask_vector, A, B, dt):\n",
        "    # bid_vector: the bid price vector, torch tensor of size N\n",
        "    # ask_vector: the ask price vector, torch tensor of size N\n",
        "    # dt: the time interval\n",
        "    # In this project, we assume that MO intensity lambda = A - B * epsilon\n",
        "\n",
        "    N = len(A)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    buy_orders = torch.zeros(N, device = device)\n",
        "    sell_orders = torch.zeros(N, device = device)\n",
        "\n",
        "    bid_intensity = A - B * bid_vector\n",
        "    ask_intensity = A - B * ask_vector\n",
        "    buy_orders = torch.distributions.poisson.Poisson(bid_intensity * dt).sample()\n",
        "    sell_orders = torch.distributions.poisson.Poisson(ask_intensity * dt).sample()\n",
        "\n",
        "    return buy_orders, sell_orders\n",
        "\n",
        "def initial_Gaussian_Policy(t, S, q, net, A, B, Q, z, delta, gamma):\n",
        "    N = len(A)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    bid_vector = torch.zeros(N, device = device)\n",
        "    ask_vector = torch.zeros(N, device = device)\n",
        "\n",
        "    mean = net.forward(t, S, q)\n",
        "    bid_mean = mean[:int(len(mean) / 2)]\n",
        "    ask_mean = mean[int(len(mean) / 2):]\n",
        "    covariance_matrix = torch.diag(gamma / (2 * z * B))\n",
        "    covariance_matrix = covariance_matrix.to(device)\n",
        "    bid_vector = torch.distributions.multivariate_normal.MultivariateNormal(bid_mean, covariance_matrix).sample()\n",
        "    ask_vector = torch.distributions.multivariate_normal.MultivariateNormal(ask_mean, covariance_matrix).sample()\n",
        "\n",
        "    return bid_vector, ask_vector\n",
        "\n",
        "def initial_Train_Data_Simulation(T, dt, sigma, S0, A, B, Q, z, delta, gamma, net):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    N = int(T / dt)\n",
        "    S = Stock_Prices_Simulation(T, dt, sigma, S0)\n",
        "    buy_orders = torch.zeros(N, len(A))\n",
        "    sell_orders = torch.zeros(N, len(A))\n",
        "    buy_orders = buy_orders.to(device)\n",
        "    sell_orders = sell_orders.to(device)\n",
        "    q = torch.zeros(N, device = device)\n",
        "    t = torch.zeros(N, device = device)\n",
        "    bid_vectors = torch.zeros(N, len(A))\n",
        "    ask_vectors = torch.zeros(N, len(A))\n",
        "    bid_vectors = bid_vectors.to(device)\n",
        "    ask_vectors = ask_vectors.to(device)\n",
        "    for i in range(N - 1):\n",
        "        bid_vector, ask_vector = initial_Gaussian_Policy(t[i], S[i], q[i], net, A, B, Q, z, delta, gamma)\n",
        "        bid_vectors[i] = bid_vector\n",
        "        ask_vectors[i] = ask_vector\n",
        "        buy_orders[i], sell_orders[i] = Market_Order_Generator(bid_vector, ask_vector, A, B, dt)\n",
        "        for j in range(len(A)):\n",
        "            q[i + 1] += (buy_orders[i][j] - sell_orders[i][j]) * z[j]\n",
        "        q[i + 1] += q[i]\n",
        "        t[i + 1] = t[i] + dt\n",
        "\n",
        "    return S, buy_orders, sell_orders, q, t, bid_vectors, ask_vectors\n",
        "\n",
        "\n",
        "def value_function_loss(net, S, q, t, dt):\n",
        "    N = len(q)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    loss = torch.zeros(N, device = device)\n",
        "    for i in range(N - 1):\n",
        "        loss[i] = (net.forward(t[i + 1], S[i + 1], q[i + 1]) - net.forward(t[i], S[i], q[i])) / dt\n",
        "    return loss\n",
        "\n",
        "def inventory_loss(net, S, q, t, dt, buy_orders, sell_orders, z, delta, Q, A, B):\n",
        "    N = len(q)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    loss = torch.zeros(N, device = device)\n",
        "    for i in range(N):\n",
        "        for k in range(len(A)):\n",
        "            loss[i] = loss[i] + (buy_orders[i][k] - sell_orders[i][k]) * (z[k] * S[i]) - delta * q[i] * q[i]\n",
        "            loss[i] = loss[i] +  (A[k] / (2 * B[k]) - (net.forward(t[i], S[i], q[i] + z[k]) - net.forward(t[i], S[i], q[i]) + z[k] * S[i]) / (2 * z[k])) * buy_orders[i][k]\n",
        "            loss[i] = loss[i] + (A[k] / (2 * B[k]) - (net.forward(t[i], S[i], q[i] - z[k]) - net.forward(t[i], S[i], q[i]) - z[k] * S[i]) / (2 * z[k])) * sell_orders[i][k]\n",
        "    return loss\n",
        "\n",
        "def total_loss(net, S, q, t, dt, buy_orders, sell_orders, z, delta, Q, A, B, gamma):\n",
        "    N = len(S)\n",
        "    K = len(A)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    loss = torch.zeros(N, device = device)\n",
        "    loss1 = value_function_loss(net, S, q, t, dt)\n",
        "    loss2 = inventory_loss(net, S, q, t, dt, buy_orders, sell_orders, z, delta, Q, A, B)\n",
        "    loss = loss1 + loss2 - gamma * ((K * 1.7981798683) + torch.sum(gamma / (2 * z * B)))\n",
        "\n",
        "    scalar_loss = 0.5 * torch.sum(loss[:-1] ** 2) * dt * dt\n",
        "    return scalar_loss"
      ],
      "metadata": {
        "id": "iFVtAvt7Ho7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = torch.tensor([1])\n",
        "dt = torch.tensor([0.01])\n",
        "S0 = torch.tensor([1])\n",
        "q = torch.tensor([0])\n",
        "A = torch.tensor([20, 18, 15, 12, 10, 8])\n",
        "B = torch.tensor([1, 1, 1, 1, 1, 1])\n",
        "Q = torch.tensor([300]) # this quantity doesn't matter since I changed the reward function\n",
        "z = torch.tensor([10, 20, 30, 40, 50, 60])\n",
        "delta = torch.tensor([0.01]) # this quantity also doesn't matter anymore\n",
        "gamma = torch.tensor([0.01])\n",
        "sigma = torch.tensor([0.05])\n",
        "\n",
        "# use cuda if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "T = T.to(device)\n",
        "dt = dt.to(device)\n",
        "S0 = S0.to(device)\n",
        "q = q.to(device)\n",
        "A = A.to(device)\n",
        "B = B.to(device)\n",
        "Q = Q.to(device)\n",
        "z = z.to(device)\n",
        "delta = delta.to(device)\n",
        "gamma = gamma.to(device)\n",
        "sigma = sigma.to(device)"
      ],
      "metadata": {
        "id": "iuVbZ6k1Hvon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_cnn_value = ResNet_Conv(3, 1, 1, 2, 1, nn.Identity()).to(device)\n",
        "res_cnn_policy = ResNet_Conv(3, 12, 1, 2, 1, nn.Sigmoid()).to(device)\n",
        "optimizer = torch.optim.Adam(res_cnn_value.parameters(), lr = 0.02)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 5, gamma = 0.95)\n",
        "loss_policy = []\n",
        "data = []\n"
      ],
      "metadata": {
        "id": "enCNAeXUHzcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(50):\n",
        "  S, buy_orders, sell_orders, q, t, bid_vector, ask_vectors = initial_Train_Data_Simulation(T, dt, sigma, S0, A, B, Q, z, delta, gamma, res_cnn_policy)\n",
        "  data.append([S, buy_orders, sell_orders, q, t, bid_vector, ask_vectors])"
      ],
      "metadata": {
        "id": "_HQDJoHFH2O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "for epoch in range(50):\n",
        "    loss = 0\n",
        "    holder = epoch % 5\n",
        "    S, buy_orders, sell_orders, q, t, bid_vectors, ask_vectors = data[random.randint(50)]\n",
        "    loss = total_loss(res_cnn_value, S, q, t, dt, buy_orders, sell_orders, z, delta, Q, A, B, gamma)\n",
        "    loss_policy.append(loss.item())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
        "    print('epoch: ', epoch, 'loss: ', loss.item() / 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2bauVlAH42u",
        "outputId": "fb7282b0-49d5-4f42-953f-43b88e151013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch-0 lr: 0.02\n",
            "epoch:  0 loss:  75536.51875\n",
            "Epoch-1 lr: 0.02\n",
            "epoch:  1 loss:  2183742.0\n",
            "Epoch-2 lr: 0.02\n",
            "epoch:  2 loss:  20915.15\n",
            "Epoch-3 lr: 0.02\n",
            "epoch:  3 loss:  40011.271875\n",
            "Epoch-4 lr: 0.019\n",
            "epoch:  4 loss:  179885.625\n",
            "Epoch-5 lr: 0.019\n",
            "epoch:  5 loss:  75566.5\n",
            "Epoch-6 lr: 0.019\n",
            "epoch:  6 loss:  297516.725\n",
            "Epoch-7 lr: 0.019\n",
            "epoch:  7 loss:  44505.715625\n",
            "Epoch-8 lr: 0.019\n",
            "epoch:  8 loss:  39785.634375\n",
            "Epoch-9 lr: 0.01805\n",
            "epoch:  9 loss:  27660.484375\n",
            "Epoch-10 lr: 0.01805\n",
            "epoch:  10 loss:  295814.275\n",
            "Epoch-11 lr: 0.01805\n",
            "epoch:  11 loss:  38913.91875\n",
            "Epoch-12 lr: 0.01805\n",
            "epoch:  12 loss:  5258.915234375\n",
            "Epoch-13 lr: 0.01805\n",
            "epoch:  13 loss:  43330.565625\n",
            "Epoch-14 lr: 0.0171475\n",
            "epoch:  14 loss:  6197.282421875\n",
            "Epoch-15 lr: 0.0171475\n",
            "epoch:  15 loss:  3336.372265625\n",
            "Epoch-16 lr: 0.0171475\n",
            "epoch:  16 loss:  2409.4267578125\n",
            "Epoch-17 lr: 0.0171475\n",
            "epoch:  17 loss:  3883.936328125\n",
            "Epoch-18 lr: 0.0171475\n",
            "epoch:  18 loss:  84322.04375\n",
            "Epoch-19 lr: 0.016290125\n",
            "epoch:  19 loss:  54619.88125\n",
            "Epoch-20 lr: 0.016290125\n",
            "epoch:  20 loss:  631.46796875\n",
            "Epoch-21 lr: 0.016290125\n",
            "epoch:  21 loss:  5170.836328125\n",
            "Epoch-22 lr: 0.016290125\n",
            "epoch:  22 loss:  3106.3564453125\n",
            "Epoch-23 lr: 0.016290125\n",
            "epoch:  23 loss:  366.9935546875\n",
            "Epoch-24 lr: 0.015475618749999998\n",
            "epoch:  24 loss:  17192.3796875\n",
            "Epoch-25 lr: 0.015475618749999998\n",
            "epoch:  25 loss:  526.029931640625\n",
            "Epoch-26 lr: 0.015475618749999998\n",
            "epoch:  26 loss:  3331.487109375\n",
            "Epoch-27 lr: 0.015475618749999998\n",
            "epoch:  27 loss:  516.242578125\n",
            "Epoch-28 lr: 0.015475618749999998\n",
            "epoch:  28 loss:  75535.69375\n",
            "Epoch-29 lr: 0.014701837812499997\n",
            "epoch:  29 loss:  62307.89375\n",
            "Epoch-30 lr: 0.014701837812499997\n",
            "epoch:  30 loss:  601585.6\n",
            "Epoch-31 lr: 0.014701837812499997\n",
            "epoch:  31 loss:  43435.959375\n",
            "Epoch-32 lr: 0.014701837812499997\n",
            "epoch:  32 loss:  17192.3796875\n",
            "Epoch-33 lr: 0.014701837812499997\n",
            "epoch:  33 loss:  5170.836328125\n",
            "Epoch-34 lr: 0.013966745921874996\n",
            "epoch:  34 loss:  34847.946875\n",
            "Epoch-35 lr: 0.013966745921874996\n",
            "epoch:  35 loss:  6176.1625\n",
            "Epoch-36 lr: 0.013966745921874996\n",
            "epoch:  36 loss:  6176.1625\n",
            "Epoch-37 lr: 0.013966745921874996\n",
            "epoch:  37 loss:  9280.13203125\n",
            "Epoch-38 lr: 0.013966745921874996\n",
            "epoch:  38 loss:  366.9935546875\n",
            "Epoch-39 lr: 0.013268408625781245\n",
            "epoch:  39 loss:  62307.89375\n",
            "Epoch-40 lr: 0.013268408625781245\n",
            "epoch:  40 loss:  2253.2732421875\n",
            "Epoch-41 lr: 0.013268408625781245\n",
            "epoch:  41 loss:  43435.959375\n",
            "Epoch-42 lr: 0.013268408625781245\n",
            "epoch:  42 loss:  2409.426953125\n",
            "Epoch-43 lr: 0.013268408625781245\n",
            "epoch:  43 loss:  17912.9953125\n",
            "Epoch-44 lr: 0.012604988194492182\n",
            "epoch:  44 loss:  44823.90625\n",
            "Epoch-45 lr: 0.012604988194492182\n",
            "epoch:  45 loss:  1121.8951171875\n",
            "Epoch-46 lr: 0.012604988194492182\n",
            "epoch:  46 loss:  278.7878662109375\n",
            "Epoch-47 lr: 0.012604988194492182\n",
            "epoch:  47 loss:  2626.9076171875\n",
            "Epoch-48 lr: 0.012604988194492182\n",
            "epoch:  48 loss:  17912.9953125\n",
            "Epoch-49 lr: 0.011974738784767573\n",
            "epoch:  49 loss:  12894.70390625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Gaussian_Policy(t, S, q, net, A, B, Q, z, delta, gamma):\n",
        "    # t: the current time\n",
        "    # S: the current stock price\n",
        "    # q: the current inventory\n",
        "    # net: the neural network\n",
        "    # This function is used to generate the bid and ask price vectors under Gaussian policy\n",
        "\n",
        "    N = len(A)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    bid_vector = torch.zeros(N, device = device)\n",
        "    ask_vector = torch.zeros(N, device = device)\n",
        "\n",
        "    for i in range(N):\n",
        "        bid_mean = (A[i] / (2 * B[i])) - (net.forward(t, S, q + z[i]) - net.forward(t, S, q) + z[i] * S ) / (2 * z[i])\n",
        "        ask_mean = (A[i] / (2 * B[i])) - (net.forward(t, S, q - z[i]) - net.forward(t, S, q) - z[i] * S ) / (2 * z[i])\n",
        "        variance = gamma / (2 * z[i] * B[i])\n",
        "        std = torch.sqrt(variance)\n",
        "        bid_vector[i] = torch.normal(bid_mean, std)\n",
        "        ask_vector[i] = torch.normal(ask_mean, std)\n",
        "\n",
        "    return bid_vector, ask_vector\n",
        "\n",
        "def Train_Data_Simulation(T, dt, sigma, S0, A, B, Q, z, delta, gamma, net):\n",
        "    # T: the total time needed\n",
        "    # dt: the time interval\n",
        "    # sigma: the volatility\n",
        "    # S0: the initial stock price\n",
        "    # net: the neural network that represent the value function\n",
        "    # this function return the simulated stock prices, buy orders, sell orders, inventory and time for N time steps\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    N = int(T / dt)\n",
        "    S = Stock_Prices_Simulation(T, dt, sigma, S0)\n",
        "    buy_orders = torch.zeros(N, len(A))\n",
        "    sell_orders = torch.zeros(N, len(A))\n",
        "    buy_orders = buy_orders.to(device)\n",
        "    sell_orders = sell_orders.to(device)\n",
        "    q = torch.zeros(N, device = device)\n",
        "    t = torch.zeros(N, device = device)\n",
        "    bid_vectors = torch.zeros(N, len(A))\n",
        "    ask_vectors = torch.zeros(N, len(A))\n",
        "    bid_vectors = bid_vectors.to(device)\n",
        "    ask_vectors = ask_vectors.to(device)\n",
        "    for i in range(N - 1):\n",
        "        bid_vector, ask_vector = Gaussian_Policy(t[i], S[i], q[i], net, A, B, Q, z, delta, gamma)\n",
        "        bid_vectors[i] = bid_vector\n",
        "        ask_vectors[i] = ask_vector\n",
        "        buy_orders[i], sell_orders[i] = Market_Order_Generator(bid_vector, ask_vector, A, B, dt)\n",
        "        for j in range(len(A)):\n",
        "            q[i + 1] += (buy_orders[i][j] - sell_orders[i][j]) * z[j]\n",
        "        q[i + 1] += q[i]\n",
        "        t[i + 1] = t[i] + dt\n",
        "\n",
        "    return S, buy_orders, sell_orders, q, t, bid_vectors, ask_vectors\n",
        "\n",
        "def final_return(S, q, buy_orders, sell_orders, bid_vectors, ask_vectors, T, dt, z):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    N = int(T / dt)\n",
        "    final_return = 0\n",
        "    for i in range(N):\n",
        "      final_return += torch.sum(z * buy_orders[i] * bid_vectors[i])\n",
        "      final_return += torch.sum(z * sell_orders[i] * ask_vectors[i])\n",
        "\n",
        "    final_return += q[-1] * S[-1]\n",
        "    return final_return"
      ],
      "metadata": {
        "id": "ZFvGJASHH7px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_net = ResNet_Conv(3, 1, 1, 2, 1, nn.Identity()).to(device)\n",
        "policy_net = res_cnn_value\n",
        "\n",
        "reward_distribution = []\n",
        "bid_mat = []\n",
        "ask_mat = []\n",
        "for iteration in range(5):\n",
        "\n",
        "  new_data = []\n",
        "  r_dis = []\n",
        "  for _ in range(50):\n",
        "    S, buy_orders, sell_orders, q, t, bid_vectors, ask_vectors = Train_Data_Simulation(T, dt, sigma, S0, A, B, Q, z, delta, gamma, policy_net)\n",
        "    new_data.append([S, buy_orders, sell_orders, q, t, bid_vectors, ask_vectors])\n",
        "    r = final_return(S, q, buy_orders, sell_orders, bid_vectors, ask_vectors, T, dt, z)\n",
        "    r_dis.append(r.item())\n",
        "\n",
        "  reward_distribution.append(r_dis)\n",
        "  optimizer = torch.optim.Adam(value_net.parameters(), lr = 0.02)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 5, gamma = 0.95)\n",
        "\n",
        "  for epoch in range(50):\n",
        "    loss = 0\n",
        "    S, buy_orders, sell_orders, q, t, bid_vectors, ask_vectors = new_data[random.randint(50)]\n",
        "    loss = total_loss(value_net, S, q, t, dt, buy_orders, sell_orders, z, delta, Q, A, B, gamma)\n",
        "    loss_policy.append(loss.item())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
        "    print('epoch: ', epoch, 'loss: ', loss.item() / 10)\n",
        "\n",
        "  policy_net = value_net\n",
        "  value_net = ResNet_Conv(3, 1, 1, 2, 1, nn.Identity()).to(device)"
      ],
      "metadata": {
        "id": "-y-zOOsBIBAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6cc67f-2c14-4746-8365-0479138b6a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch-0 lr: 0.02\n",
            "epoch:  0 loss:  166134.875\n",
            "Epoch-1 lr: 0.02\n",
            "epoch:  1 loss:  774579712.0\n",
            "Epoch-2 lr: 0.02\n",
            "epoch:  2 loss:  78514.58125\n",
            "Epoch-3 lr: 0.02\n",
            "epoch:  3 loss:  137388.9875\n",
            "Epoch-4 lr: 0.019\n",
            "epoch:  4 loss:  117259.55\n",
            "Epoch-5 lr: 0.019\n",
            "epoch:  5 loss:  30521.01875\n",
            "Epoch-6 lr: 0.019\n",
            "epoch:  6 loss:  18980.9390625\n",
            "Epoch-7 lr: 0.019\n",
            "epoch:  7 loss:  67243.2375\n",
            "Epoch-8 lr: 0.019\n",
            "epoch:  8 loss:  337520.25\n",
            "Epoch-9 lr: 0.01805\n",
            "epoch:  9 loss:  11290.0890625\n",
            "Epoch-10 lr: 0.01805\n",
            "epoch:  10 loss:  108307.975\n",
            "Epoch-11 lr: 0.01805\n",
            "epoch:  11 loss:  29507.7\n",
            "Epoch-12 lr: 0.01805\n",
            "epoch:  12 loss:  602114.65\n",
            "Epoch-13 lr: 0.01805\n",
            "epoch:  13 loss:  68205.175\n",
            "Epoch-14 lr: 0.0171475\n",
            "epoch:  14 loss:  77853.25625\n",
            "Epoch-15 lr: 0.0171475\n",
            "epoch:  15 loss:  78831.11875\n",
            "Epoch-16 lr: 0.0171475\n",
            "epoch:  16 loss:  23465.709375\n",
            "Epoch-17 lr: 0.0171475\n",
            "epoch:  17 loss:  638986.1\n",
            "Epoch-18 lr: 0.0171475\n",
            "epoch:  18 loss:  8091.12109375\n",
            "Epoch-19 lr: 0.016290125\n",
            "epoch:  19 loss:  7582.615625\n",
            "Epoch-20 lr: 0.016290125\n",
            "epoch:  20 loss:  20179.1078125\n",
            "Epoch-21 lr: 0.016290125\n",
            "epoch:  21 loss:  103250.63125\n",
            "Epoch-22 lr: 0.016290125\n",
            "epoch:  22 loss:  107926.325\n",
            "Epoch-23 lr: 0.016290125\n",
            "epoch:  23 loss:  34283.034375\n",
            "Epoch-24 lr: 0.015475618749999998\n",
            "epoch:  24 loss:  2945.27890625\n",
            "Epoch-25 lr: 0.015475618749999998\n",
            "epoch:  25 loss:  105887.125\n",
            "Epoch-26 lr: 0.015475618749999998\n",
            "epoch:  26 loss:  3680.0703125\n",
            "Epoch-27 lr: 0.015475618749999998\n",
            "epoch:  27 loss:  5330.494921875\n",
            "Epoch-28 lr: 0.015475618749999998\n",
            "epoch:  28 loss:  611.242529296875\n",
            "Epoch-29 lr: 0.014701837812499997\n",
            "epoch:  29 loss:  827.5220703125\n",
            "Epoch-30 lr: 0.014701837812499997\n",
            "epoch:  30 loss:  1268.3154296875\n",
            "Epoch-31 lr: 0.014701837812499997\n",
            "epoch:  31 loss:  4797.3515625\n",
            "Epoch-32 lr: 0.014701837812499997\n",
            "epoch:  32 loss:  680.77734375\n",
            "Epoch-33 lr: 0.014701837812499997\n",
            "epoch:  33 loss:  34569.803125\n",
            "Epoch-34 lr: 0.013966745921874996\n",
            "epoch:  34 loss:  184918.3125\n",
            "Epoch-35 lr: 0.013966745921874996\n",
            "epoch:  35 loss:  728.990087890625\n",
            "Epoch-36 lr: 0.013966745921874996\n",
            "epoch:  36 loss:  966.9677734375\n",
            "Epoch-37 lr: 0.013966745921874996\n",
            "epoch:  37 loss:  60270.6375\n",
            "Epoch-38 lr: 0.013966745921874996\n",
            "epoch:  38 loss:  1753.275\n",
            "Epoch-39 lr: 0.013268408625781245\n",
            "epoch:  39 loss:  20280.071875\n",
            "Epoch-40 lr: 0.013268408625781245\n",
            "epoch:  40 loss:  863.81435546875\n",
            "Epoch-41 lr: 0.013268408625781245\n",
            "epoch:  41 loss:  636.96083984375\n",
            "Epoch-42 lr: 0.013268408625781245\n",
            "epoch:  42 loss:  16606.475\n",
            "Epoch-43 lr: 0.013268408625781245\n",
            "epoch:  43 loss:  20796.2\n",
            "Epoch-44 lr: 0.012604988194492182\n",
            "epoch:  44 loss:  34544.384375\n",
            "Epoch-45 lr: 0.012604988194492182\n",
            "epoch:  45 loss:  176629.6\n",
            "Epoch-46 lr: 0.012604988194492182\n",
            "epoch:  46 loss:  395.9623046875\n",
            "Epoch-47 lr: 0.012604988194492182\n",
            "epoch:  47 loss:  2061.10703125\n",
            "Epoch-48 lr: 0.012604988194492182\n",
            "epoch:  48 loss:  21912.33125\n",
            "Epoch-49 lr: 0.011974738784767573\n",
            "epoch:  49 loss:  42.4109375\n",
            "Epoch-0 lr: 0.02\n",
            "epoch:  0 loss:  20961.146875\n",
            "Epoch-1 lr: 0.02\n",
            "epoch:  1 loss:  6746451.2\n",
            "Epoch-2 lr: 0.02\n",
            "epoch:  2 loss:  153612.6375\n",
            "Epoch-3 lr: 0.02\n",
            "epoch:  3 loss:  43465.603125\n",
            "Epoch-4 lr: 0.019\n",
            "epoch:  4 loss:  145033.7625\n",
            "Epoch-5 lr: 0.019\n",
            "epoch:  5 loss:  240006.5\n",
            "Epoch-6 lr: 0.019\n",
            "epoch:  6 loss:  76833.53125\n",
            "Epoch-7 lr: 0.019\n",
            "epoch:  7 loss:  45440.35625\n",
            "Epoch-8 lr: 0.019\n",
            "epoch:  8 loss:  132518.675\n",
            "Epoch-9 lr: 0.01805\n",
            "epoch:  9 loss:  215375.675\n",
            "Epoch-10 lr: 0.01805\n",
            "epoch:  10 loss:  120763.0875\n",
            "Epoch-11 lr: 0.01805\n",
            "epoch:  11 loss:  72630.425\n",
            "Epoch-12 lr: 0.01805\n",
            "epoch:  12 loss:  79800.9\n",
            "Epoch-13 lr: 0.01805\n",
            "epoch:  13 loss:  56984.6375\n",
            "Epoch-14 lr: 0.0171475\n",
            "epoch:  14 loss:  3643.657421875\n",
            "Epoch-15 lr: 0.0171475\n",
            "epoch:  15 loss:  7524.59453125\n",
            "Epoch-16 lr: 0.0171475\n",
            "epoch:  16 loss:  11181.96796875\n",
            "Epoch-17 lr: 0.0171475\n",
            "epoch:  17 loss:  378148.5\n",
            "Epoch-18 lr: 0.0171475\n",
            "epoch:  18 loss:  34494.103125\n",
            "Epoch-19 lr: 0.016290125\n",
            "epoch:  19 loss:  74439.20625\n",
            "Epoch-20 lr: 0.016290125\n",
            "epoch:  20 loss:  2592.874609375\n",
            "Epoch-21 lr: 0.016290125\n",
            "epoch:  21 loss:  376997.625\n",
            "Epoch-22 lr: 0.016290125\n",
            "epoch:  22 loss:  840.11259765625\n",
            "Epoch-23 lr: 0.016290125\n",
            "epoch:  23 loss:  14856.6046875\n",
            "Epoch-24 lr: 0.015475618749999998\n",
            "epoch:  24 loss:  2802.84453125\n",
            "Epoch-25 lr: 0.015475618749999998\n",
            "epoch:  25 loss:  142724.375\n",
            "Epoch-26 lr: 0.015475618749999998\n",
            "epoch:  26 loss:  27246.740625\n",
            "Epoch-27 lr: 0.015475618749999998\n",
            "epoch:  27 loss:  18546.525\n",
            "Epoch-28 lr: 0.015475618749999998\n",
            "epoch:  28 loss:  47.1084716796875\n",
            "Epoch-29 lr: 0.014701837812499997\n",
            "epoch:  29 loss:  915.80458984375\n",
            "Epoch-30 lr: 0.014701837812499997\n",
            "epoch:  30 loss:  67953.65625\n",
            "Epoch-31 lr: 0.014701837812499997\n",
            "epoch:  31 loss:  855.8900390625\n",
            "Epoch-32 lr: 0.014701837812499997\n",
            "epoch:  32 loss:  2011.3833984375\n",
            "Epoch-33 lr: 0.014701837812499997\n",
            "epoch:  33 loss:  32298.865625\n",
            "Epoch-34 lr: 0.013966745921874996\n",
            "epoch:  34 loss:  9023.58046875\n",
            "Epoch-35 lr: 0.013966745921874996\n",
            "epoch:  35 loss:  991.6466796875\n",
            "Epoch-36 lr: 0.013966745921874996\n",
            "epoch:  36 loss:  2159.125\n",
            "Epoch-37 lr: 0.013966745921874996\n",
            "epoch:  37 loss:  33947.503125\n",
            "Epoch-38 lr: 0.013966745921874996\n",
            "epoch:  38 loss:  120797.6875\n",
            "Epoch-39 lr: 0.013268408625781245\n",
            "epoch:  39 loss:  72801.89375\n",
            "Epoch-40 lr: 0.013268408625781245\n",
            "epoch:  40 loss:  6714.19609375\n",
            "Epoch-41 lr: 0.013268408625781245\n",
            "epoch:  41 loss:  85487.04375\n",
            "Epoch-42 lr: 0.013268408625781245\n",
            "epoch:  42 loss:  88.85682373046875\n",
            "Epoch-43 lr: 0.013268408625781245\n",
            "epoch:  43 loss:  47.411233520507814\n",
            "Epoch-44 lr: 0.012604988194492182\n",
            "epoch:  44 loss:  14940.05625\n",
            "Epoch-45 lr: 0.012604988194492182\n",
            "epoch:  45 loss:  2098.7125\n",
            "Epoch-46 lr: 0.012604988194492182\n",
            "epoch:  46 loss:  47.88465270996094\n",
            "Epoch-47 lr: 0.012604988194492182\n",
            "epoch:  47 loss:  4987.733203125\n",
            "Epoch-48 lr: 0.012604988194492182\n",
            "epoch:  48 loss:  30724.509375\n",
            "Epoch-49 lr: 0.011974738784767573\n",
            "epoch:  49 loss:  11760.06875\n",
            "Epoch-0 lr: 0.02\n",
            "epoch:  0 loss:  209433.375\n",
            "Epoch-1 lr: 0.02\n",
            "epoch:  1 loss:  105349094.4\n",
            "Epoch-2 lr: 0.02\n",
            "epoch:  2 loss:  4564.979296875\n",
            "Epoch-3 lr: 0.02\n",
            "epoch:  3 loss:  120244.4875\n",
            "Epoch-4 lr: 0.019\n",
            "epoch:  4 loss:  146906.625\n",
            "Epoch-5 lr: 0.019\n",
            "epoch:  5 loss:  1884.158984375\n",
            "Epoch-6 lr: 0.019\n",
            "epoch:  6 loss:  5478.266015625\n",
            "Epoch-7 lr: 0.019\n",
            "epoch:  7 loss:  1080.8388671875\n",
            "Epoch-8 lr: 0.019\n",
            "epoch:  8 loss:  208457.6\n",
            "Epoch-9 lr: 0.01805\n",
            "epoch:  9 loss:  12176.05234375\n",
            "Epoch-10 lr: 0.01805\n",
            "epoch:  10 loss:  9476.2578125\n",
            "Epoch-11 lr: 0.01805\n",
            "epoch:  11 loss:  4632.973828125\n",
            "Epoch-12 lr: 0.01805\n",
            "epoch:  12 loss:  3274.8728515625\n",
            "Epoch-13 lr: 0.01805\n",
            "epoch:  13 loss:  44160.05625\n",
            "Epoch-14 lr: 0.0171475\n",
            "epoch:  14 loss:  11710.271875\n",
            "Epoch-15 lr: 0.0171475\n",
            "epoch:  15 loss:  992.29453125\n",
            "Epoch-16 lr: 0.0171475\n",
            "epoch:  16 loss:  10419.63359375\n",
            "Epoch-17 lr: 0.0171475\n",
            "epoch:  17 loss:  2834.8234375\n",
            "Epoch-18 lr: 0.0171475\n",
            "epoch:  18 loss:  424.628076171875\n",
            "Epoch-19 lr: 0.016290125\n",
            "epoch:  19 loss:  1978.594921875\n",
            "Epoch-20 lr: 0.016290125\n",
            "epoch:  20 loss:  26587.53125\n",
            "Epoch-21 lr: 0.016290125\n",
            "epoch:  21 loss:  31302.940625\n",
            "Epoch-22 lr: 0.016290125\n",
            "epoch:  22 loss:  1110.37109375\n",
            "Epoch-23 lr: 0.016290125\n",
            "epoch:  23 loss:  47805.80625\n",
            "Epoch-24 lr: 0.015475618749999998\n",
            "epoch:  24 loss:  210159.4\n",
            "Epoch-25 lr: 0.015475618749999998\n",
            "epoch:  25 loss:  2608.84140625\n",
            "Epoch-26 lr: 0.015475618749999998\n",
            "epoch:  26 loss:  70736.66875\n",
            "Epoch-27 lr: 0.015475618749999998\n",
            "epoch:  27 loss:  120028.5125\n",
            "Epoch-28 lr: 0.015475618749999998\n",
            "epoch:  28 loss:  994.9611328125\n",
            "Epoch-29 lr: 0.014701837812499997\n",
            "epoch:  29 loss:  5344.94375\n",
            "Epoch-30 lr: 0.014701837812499997\n",
            "epoch:  30 loss:  47939.6\n",
            "Epoch-31 lr: 0.014701837812499997\n",
            "epoch:  31 loss:  147411.7625\n",
            "Epoch-32 lr: 0.014701837812499997\n",
            "epoch:  32 loss:  12941.96171875\n",
            "Epoch-33 lr: 0.014701837812499997\n",
            "epoch:  33 loss:  1834.1142578125\n",
            "Epoch-34 lr: 0.013966745921874996\n",
            "epoch:  34 loss:  125.825\n",
            "Epoch-35 lr: 0.013966745921874996\n",
            "epoch:  35 loss:  13292.8015625\n",
            "Epoch-36 lr: 0.013966745921874996\n",
            "epoch:  36 loss:  1734.5234375\n",
            "Epoch-37 lr: 0.013966745921874996\n",
            "epoch:  37 loss:  70508.1125\n",
            "Epoch-38 lr: 0.013966745921874996\n",
            "epoch:  38 loss:  1469.76455078125\n",
            "Epoch-39 lr: 0.013268408625781245\n",
            "epoch:  39 loss:  668.564404296875\n",
            "Epoch-40 lr: 0.013268408625781245\n",
            "epoch:  40 loss:  37408.05\n",
            "Epoch-41 lr: 0.013268408625781245\n",
            "epoch:  41 loss:  196.957958984375\n",
            "Epoch-42 lr: 0.013268408625781245\n",
            "epoch:  42 loss:  445.7\n",
            "Epoch-43 lr: 0.013268408625781245\n",
            "epoch:  43 loss:  208808.4625\n",
            "Epoch-44 lr: 0.012604988194492182\n",
            "epoch:  44 loss:  4799.95\n",
            "Epoch-45 lr: 0.012604988194492182\n",
            "epoch:  45 loss:  1801.6359375\n",
            "Epoch-46 lr: 0.012604988194492182\n",
            "epoch:  46 loss:  207.33271484375\n",
            "Epoch-47 lr: 0.012604988194492182\n",
            "epoch:  47 loss:  156.025048828125\n",
            "Epoch-48 lr: 0.012604988194492182\n",
            "epoch:  48 loss:  972.0177734375\n",
            "Epoch-49 lr: 0.011974738784767573\n",
            "epoch:  49 loss:  146924.825\n",
            "Epoch-0 lr: 0.02\n",
            "epoch:  0 loss:  12391.00859375\n",
            "Epoch-1 lr: 0.02\n",
            "epoch:  1 loss:  20986393.6\n",
            "Epoch-2 lr: 0.02\n",
            "epoch:  2 loss:  2294244.6\n",
            "Epoch-3 lr: 0.02\n",
            "epoch:  3 loss:  1040.8173828125\n",
            "Epoch-4 lr: 0.019\n",
            "epoch:  4 loss:  3832.691796875\n",
            "Epoch-5 lr: 0.019\n",
            "epoch:  5 loss:  283181.125\n",
            "Epoch-6 lr: 0.019\n",
            "epoch:  6 loss:  74971.95\n",
            "Epoch-7 lr: 0.019\n",
            "epoch:  7 loss:  163400.225\n",
            "Epoch-8 lr: 0.019\n",
            "epoch:  8 loss:  68058.44375\n",
            "Epoch-9 lr: 0.01805\n",
            "epoch:  9 loss:  4584.454296875\n",
            "Epoch-10 lr: 0.01805\n",
            "epoch:  10 loss:  4806.9625\n",
            "Epoch-11 lr: 0.01805\n",
            "epoch:  11 loss:  11865.359375\n",
            "Epoch-12 lr: 0.01805\n",
            "epoch:  12 loss:  3147.7119140625\n",
            "Epoch-13 lr: 0.01805\n",
            "epoch:  13 loss:  8702.8890625\n",
            "Epoch-14 lr: 0.0171475\n",
            "epoch:  14 loss:  1861.008203125\n",
            "Epoch-15 lr: 0.0171475\n",
            "epoch:  15 loss:  8443.4171875\n",
            "Epoch-16 lr: 0.0171475\n",
            "epoch:  16 loss:  2063.930859375\n",
            "Epoch-17 lr: 0.0171475\n",
            "epoch:  17 loss:  33522.76875\n",
            "Epoch-18 lr: 0.0171475\n",
            "epoch:  18 loss:  26241.959375\n",
            "Epoch-19 lr: 0.016290125\n",
            "epoch:  19 loss:  301875.0\n",
            "Epoch-20 lr: 0.016290125\n",
            "epoch:  20 loss:  9643.9265625\n",
            "Epoch-21 lr: 0.016290125\n",
            "epoch:  21 loss:  33753.734375\n",
            "Epoch-22 lr: 0.016290125\n",
            "epoch:  22 loss:  1923.0150390625\n",
            "Epoch-23 lr: 0.016290125\n",
            "epoch:  23 loss:  34534.859375\n",
            "Epoch-24 lr: 0.015475618749999998\n",
            "epoch:  24 loss:  1415.1310546875\n",
            "Epoch-25 lr: 0.015475618749999998\n",
            "epoch:  25 loss:  146185.575\n",
            "Epoch-26 lr: 0.015475618749999998\n",
            "epoch:  26 loss:  23948.0609375\n",
            "Epoch-27 lr: 0.015475618749999998\n",
            "epoch:  27 loss:  300059.925\n",
            "Epoch-28 lr: 0.015475618749999998\n",
            "epoch:  28 loss:  6063.97421875\n",
            "Epoch-29 lr: 0.014701837812499997\n",
            "epoch:  29 loss:  4265.1328125\n",
            "Epoch-30 lr: 0.014701837812499997\n",
            "epoch:  30 loss:  6465.07421875\n",
            "Epoch-31 lr: 0.014701837812499997\n",
            "epoch:  31 loss:  73638.6625\n",
            "Epoch-32 lr: 0.014701837812499997\n",
            "epoch:  32 loss:  74219.0375\n",
            "Epoch-33 lr: 0.014701837812499997\n",
            "epoch:  33 loss:  636.814306640625\n",
            "Epoch-34 lr: 0.013966745921874996\n",
            "epoch:  34 loss:  3439.07578125\n",
            "Epoch-35 lr: 0.013966745921874996\n",
            "epoch:  35 loss:  57230.15\n",
            "Epoch-36 lr: 0.013966745921874996\n",
            "epoch:  36 loss:  7708.53671875\n",
            "Epoch-37 lr: 0.013966745921874996\n",
            "epoch:  37 loss:  234178.75\n",
            "Epoch-38 lr: 0.013966745921874996\n",
            "epoch:  38 loss:  1268.92958984375\n",
            "Epoch-39 lr: 0.013268408625781245\n",
            "epoch:  39 loss:  7403.25546875\n",
            "Epoch-40 lr: 0.013268408625781245\n",
            "epoch:  40 loss:  1673.71875\n",
            "Epoch-41 lr: 0.013268408625781245\n",
            "epoch:  41 loss:  897.87919921875\n",
            "Epoch-42 lr: 0.013268408625781245\n",
            "epoch:  42 loss:  3041.5427734375\n",
            "Epoch-43 lr: 0.013268408625781245\n",
            "epoch:  43 loss:  3043.0654296875\n",
            "Epoch-44 lr: 0.012604988194492182\n",
            "epoch:  44 loss:  1686.7048828125\n",
            "Epoch-45 lr: 0.012604988194492182\n",
            "epoch:  45 loss:  139.67159423828124\n",
            "Epoch-46 lr: 0.012604988194492182\n",
            "epoch:  46 loss:  232993.425\n",
            "Epoch-47 lr: 0.012604988194492182\n",
            "epoch:  47 loss:  3416.691796875\n",
            "Epoch-48 lr: 0.012604988194492182\n",
            "epoch:  48 loss:  57266.85625\n",
            "Epoch-49 lr: 0.011974738784767573\n",
            "epoch:  49 loss:  8489.5109375\n",
            "Epoch-0 lr: 0.02\n",
            "epoch:  0 loss:  1980.7083984375\n",
            "Epoch-1 lr: 0.02\n",
            "epoch:  1 loss:  3391.556640625\n",
            "Epoch-2 lr: 0.02\n",
            "epoch:  2 loss:  12498.14765625\n",
            "Epoch-3 lr: 0.02\n",
            "epoch:  3 loss:  10650.22578125\n",
            "Epoch-4 lr: 0.019\n",
            "epoch:  4 loss:  211.457080078125\n",
            "Epoch-5 lr: 0.019\n",
            "epoch:  5 loss:  10647.55859375\n",
            "Epoch-6 lr: 0.019\n",
            "epoch:  6 loss:  842.030078125\n",
            "Epoch-7 lr: 0.019\n",
            "epoch:  7 loss:  7848.428125\n",
            "Epoch-8 lr: 0.019\n",
            "epoch:  8 loss:  419.8330078125\n",
            "Epoch-9 lr: 0.01805\n",
            "epoch:  9 loss:  1980.4681640625\n",
            "Epoch-10 lr: 0.01805\n",
            "epoch:  10 loss:  419.4666015625\n",
            "Epoch-11 lr: 0.01805\n",
            "epoch:  11 loss:  835.66552734375\n",
            "Epoch-12 lr: 0.01805\n",
            "epoch:  12 loss:  76.7410400390625\n",
            "Epoch-13 lr: 0.01805\n",
            "epoch:  13 loss:  40.06773071289062\n",
            "Epoch-14 lr: 0.0171475\n",
            "epoch:  14 loss:  1290.20869140625\n",
            "Epoch-15 lr: 0.0171475\n",
            "epoch:  15 loss:  35052.090625\n",
            "Epoch-16 lr: 0.0171475\n",
            "epoch:  16 loss:  259438.825\n",
            "Epoch-17 lr: 0.0171475\n",
            "epoch:  17 loss:  2467.3388671875\n",
            "Epoch-18 lr: 0.0171475\n",
            "epoch:  18 loss:  3313.936328125\n",
            "Epoch-19 lr: 0.016290125\n",
            "epoch:  19 loss:  44920.871875\n",
            "Epoch-20 lr: 0.016290125\n",
            "epoch:  20 loss:  9337.990625\n",
            "Epoch-21 lr: 0.016290125\n",
            "epoch:  21 loss:  2521.3884765625\n",
            "Epoch-22 lr: 0.016290125\n",
            "epoch:  22 loss:  222.973046875\n",
            "Epoch-23 lr: 0.016290125\n",
            "epoch:  23 loss:  25868.3015625\n",
            "Epoch-24 lr: 0.015475618749999998\n",
            "epoch:  24 loss:  1420.87392578125\n",
            "Epoch-25 lr: 0.015475618749999998\n",
            "epoch:  25 loss:  3695.21015625\n",
            "Epoch-26 lr: 0.015475618749999998\n",
            "epoch:  26 loss:  6038.158203125\n",
            "Epoch-27 lr: 0.015475618749999998\n",
            "epoch:  27 loss:  2467.3388671875\n",
            "Epoch-28 lr: 0.015475618749999998\n",
            "epoch:  28 loss:  35052.090625\n",
            "Epoch-29 lr: 0.014701837812499997\n",
            "epoch:  29 loss:  61218.7125\n",
            "Epoch-30 lr: 0.014701837812499997\n",
            "epoch:  30 loss:  12483.16953125\n",
            "Epoch-31 lr: 0.014701837812499997\n",
            "epoch:  31 loss:  835.6818359375\n",
            "Epoch-32 lr: 0.014701837812499997\n",
            "epoch:  32 loss:  12483.16953125\n",
            "Epoch-33 lr: 0.014701837812499997\n",
            "epoch:  33 loss:  2521.3884765625\n",
            "Epoch-34 lr: 0.013966745921874996\n",
            "epoch:  34 loss:  164022.9625\n",
            "Epoch-35 lr: 0.013966745921874996\n",
            "epoch:  35 loss:  1706.8021484375\n",
            "Epoch-36 lr: 0.013966745921874996\n",
            "epoch:  36 loss:  44914.675\n",
            "Epoch-37 lr: 0.013966745921874996\n",
            "epoch:  37 loss:  35183.946875\n",
            "Epoch-38 lr: 0.013966745921874996\n",
            "epoch:  38 loss:  3313.936328125\n",
            "Epoch-39 lr: 0.013268408625781245\n",
            "epoch:  39 loss:  3382.74921875\n",
            "Epoch-40 lr: 0.013268408625781245\n",
            "epoch:  40 loss:  3382.74921875\n",
            "Epoch-41 lr: 0.013268408625781245\n",
            "epoch:  41 loss:  131.57293701171875\n",
            "Epoch-42 lr: 0.013268408625781245\n",
            "epoch:  42 loss:  1980.701171875\n",
            "Epoch-43 lr: 0.013268408625781245\n",
            "epoch:  43 loss:  55056.84375\n",
            "Epoch-44 lr: 0.012604988194492182\n",
            "epoch:  44 loss:  9337.990625\n",
            "Epoch-45 lr: 0.012604988194492182\n",
            "epoch:  45 loss:  35052.090625\n",
            "Epoch-46 lr: 0.012604988194492182\n",
            "epoch:  46 loss:  9337.990625\n",
            "Epoch-47 lr: 0.012604988194492182\n",
            "epoch:  47 loss:  12483.16953125\n",
            "Epoch-48 lr: 0.012604988194492182\n",
            "epoch:  48 loss:  352.036669921875\n",
            "Epoch-49 lr: 0.011974738784767573\n",
            "epoch:  49 loss:  25868.3015625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_mat = torch.tensor([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]).to(device)\n",
        "S_mat = torch.tensor([0.9, 0.92, 0.94, 0.96, 0.98, 1, 1.02, 1.04, 1.06, 1.08]).to(device)\n",
        "q_mat = torch.tensor([50, 100, 150, 200, 250, 300, 350, 400, 450, 500]).to(device)\n",
        "bid_mat = torch.zeros(10, 10, 10, 6).to(device)\n",
        "ask_mat = torch.zeros(10, 10, 10, 6).to(device)\n",
        "\n",
        "def bid_ask_mean(t, S, q, net, A, B, Q, z, delta, gamma):\n",
        "    # t: the current time\n",
        "    # S: the current stock price\n",
        "    # q: the current inventory\n",
        "    # net: the neural network\n",
        "    # This function is used to generate the bid and ask price vectors under Gaussian policy\n",
        "\n",
        "    N = len(A)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    bid_vector = torch.zeros(N, device = device)\n",
        "    ask_vector = torch.zeros(N, device = device)\n",
        "\n",
        "    for i in range(N):\n",
        "        bid_mean = (A[i] / (2 * B[i])) - (net.forward(t, S, q + z[i]) - net.forward(t, S, q) + z[i] * S ) / (2 * z[i])\n",
        "        ask_mean = (A[i] / (2 * B[i])) - (net.forward(t, S, q - z[i]) - net.forward(t, S, q) - z[i] * S ) / (2 * z[i])\n",
        "        bid_vector[i] = bid_mean\n",
        "        ask_vector[i] = ask_mean\n",
        "\n",
        "    return bid_vector, ask_vector\n",
        "\n",
        "for i in range(1):\n",
        "  for j in range(1):\n",
        "    for k in range(1):\n",
        "      bid_vector, ask_vector = bid_ask_mean(t_mat[i], S_mat[j], q_mat[k], policy_net, A, B, Q, z, delta, gamma)\n",
        "      bid_mat[i][j][k] = bid_vector\n",
        "      ask_mat[i][j][k] = ask_vector\n",
        "\n",
        "\n",
        "print(bid_mat[0][0][0])\n",
        "print(ask_mat[0][0][0])\n",
        "\n",
        "print(bid_ask_mean(t_mat[0], S_mat[0], q_mat[0], res_cnn_value, A, B, Q, z, delta, gamma))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdAu3b5MZX73",
        "outputId": "788fc71b-38a9-45f0-f9f3-70700874748f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9.5500, 8.5500, 7.0500, 5.5500, 4.5500, 3.5500], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([10.4500,  9.4500,  7.9500,  6.4500,  5.4500,  4.4500], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n",
            "(tensor([9.5500, 8.5500, 7.0500, 5.5500, 4.5500, 3.5500], device='cuda:0',\n",
            "       grad_fn=<CopySlices>), tensor([10.4500,  9.4500,  7.9500,  6.4500,  5.4500,  4.4500], device='cuda:0',\n",
            "       grad_fn=<CopySlices>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = np.array(reward_distribution)\n",
        "d = bid_mat.cpu().detach().numpy()\n",
        "e = ask_mat.cpu().detach().numpy()\n",
        "np.save('reward_distribution_policy_iteration.npy', c)\n",
        "np.save('bid_mat_policy_iteration.npy', d)\n",
        "np.save('ask_mat_policy_iteration.npy', e)"
      ],
      "metadata": {
        "id": "GHwt2DNGZYkf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}